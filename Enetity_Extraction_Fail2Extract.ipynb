{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_3dIZ2_dqFS",
        "outputId": "86675184-a3e4-47f2-d520-12eef8de2387"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLTK"
      ],
      "metadata": {
        "id": "CL_P1Sutpjmt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_y6srF4wcKRx"
      },
      "outputs": [],
      "source": [
        "#solution NLTK\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "from nltk import Tree\n",
        "def get_continuous_chunks(text):\n",
        "  chunked = ne_chunk(pos_tag(word_tokenize(text)))\n",
        "  prev = None\n",
        "  continuous_chunk = []\n",
        "  current_chunk = []\n",
        "  for subtree in chunked:\n",
        "    # if type(subtree) == Tree and subtree.label() == label:\n",
        "    if type(subtree) == Tree:\n",
        "      l=subtree.label()\n",
        "      current_chunk.append(' '.join([token for token, pos in subtree.leaves()]))\n",
        "    elif current_chunk:\n",
        "      named_entity = ' '.join(current_chunk)\n",
        "      if named_entity not in continuous_chunk:\n",
        "        continuous_chunk.append((l,named_entity))\n",
        "        current_chunk = []\n",
        "      else:\n",
        "        continue\n",
        "  return continuous_chunk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'This agreement is made and entered into by and between \"Abc & Co.\" and \"Bcd LLC\" for term 1 year starting from April 1, 2020, hereinafter collectively referred to as the Parties. Samuel P. Jackson in the place (New York) and on the date written below, with the following terms and conditions.'"
      ],
      "metadata": {
        "id": "sChNbYU-dPCx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "continuous_chunk = get_continuous_chunks(text)"
      ],
      "metadata": {
        "id": "RGXF_CJNdWAR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for entity in get_continuous_chunks(text):\n",
        "  print(entity[0], ' — — — ', entity[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-sDOaW0gfzg",
        "outputId": "9fcdf605-21fb-4647-e48b-4a1b3a362da0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PERSON  — — —  Bcd LLC\n",
            "ORGANIZATION  — — —  Parties\n",
            "PERSON  — — —  Samuel P. Jackson\n",
            "GPE  — — —  New York\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = 'The Little Prince.txt'\n",
        "dict_nltk = {}\n",
        "with open(file, \"r\") as file1:\n",
        "    FileContent = file1.read()\n",
        "    # print(len(FileContent)) # 79779\n",
        "    for entity in get_continuous_chunks(FileContent):\n",
        "      print(entity[0], ' — — — ', entity[1])\n",
        "\n",
        "      if entity[0] not in dict_nltk.keys():\n",
        "        dict_nltk[entity[0]] = [entity[1]]\n",
        "      else:\n",
        "        if entity[1] not in dict_nltk[entity[0]]:\n",
        "          dict_nltk[entity[0]].append(entity[1])"
      ],
      "metadata": {
        "id": "JplKyB0CfZNW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebaf3e8e-59fc-49d3-9b7a-1db1e68d7a02"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ORGANIZATION  — — —  LITTLE\n",
            "PERSON  — — —  Scared\n",
            "PERSON  — — —  Hence\n",
            "GPE  — — —  China\n",
            "GPE  — — —  Arizona\n",
            "ORGANIZATION  — — —  Sahara\n",
            "PERSON  — — —  Miles\n",
            "PERSON  — — —  Utterly\n",
            "PERSON  — — —  See\n",
            "PERSON  — — —  Earth\n",
            "PERSON  — — —  Jupiter\n",
            "PERSON  — — —  Mars\n",
            "PERSON  — — —  Venus\n",
            "GPE  — — —  Turkish\n",
            "ORGANIZATION  — — —  International Astronomical Congress\n",
            "GPE  — — —  Turkish\n",
            "ORGANIZATION  — — —  Asteroid\n",
            "GPE  — — —  Turkish\n",
            "GPE  — — —  European\n",
            "PERSON  — — —  How\n",
            "PERSON  — — —  Asteroid B-612\n",
            "PERSON  — — —  Children\n",
            "GPE  — — —  Children\n",
            "GPE  — — —  America\n",
            "GPE  — — —  France\n",
            "GPE  — — —  France\n",
            "GPE  — — —  France\n",
            "PERSON  — — —  Don\n",
            "PERSON  — — —  Earth\n",
            "PERSON  — — —  Goodbye\n",
            "PERSON  — — —  Don\n",
            "ORGANIZATION  — — —  Come\n",
            "PERSON  — — —  Him\n",
            "PERSON  — — —  Ashamed\n",
            "PERSON  — — —  Twelve\n",
            "PERSON  — — —  How\n",
            "PERSON  — — —  Will\n",
            "PERSON  — — —  Which\n",
            "PERSON  — — —  Earth\n",
            "PERSON  — — —  Earth\n",
            "ORGANIZATION  — — —  Earth\n",
            "ORGANIZATION  — — —  African\n",
            "LOCATION  — — —  Earth\n",
            "GPE  — — —  New Zealand\n",
            "GPE  — — —  Australia\n",
            "GPE  — — —  China\n",
            "PERSON  — — —  Siberia\n",
            "GPE  — — —  Russia\n",
            "GPE  — — —  India\n",
            "GPE  — — —  Africa\n",
            "GPE  — — —  Europe\n",
            "GPE  — — —  South America\n",
            "GPE  — — —  North America\n",
            "LOCATION  — — —  North Pole\n",
            "LOCATION  — — —  South Pole\n",
            "LOCATION  — — —  Earth\n",
            "GPE  — — —  Pacific\n",
            "GPE  — — —  Earth\n",
            "ORGANIZATION  — — —  Good\n",
            "ORGANIZATION  — — —  Good\n",
            "LOCATION  — — —  Earth\n",
            "GPE  — — —  Africa\n",
            "GPE  — — —  Earth\n",
            "ORGANIZATION  — — —  Earth\n",
            "PERSON  — — —  Look\n",
            "LOCATION  — — —  Earth\n",
            "PERSON  — — —  Hello\n",
            "GPE  — — —  Greetings\n",
            "PERSON  — — —  Hello\n",
            "PERSON  — — —  Hello\n",
            "ORGANIZATION  — — —  Come\n",
            "PERSON  — — —  Correct\n",
            "PERSON  — — —  Wheat\n",
            "PERSON  — — —  Aren\n",
            "PERSON  — — —  Hello\n",
            "PERSON  — — —  Thirst\n",
            "LOCATION  — — —  Earth\n",
            "GPE  — — —  Sahara\n",
            "ORGANIZATION  — — —  Christmas\n",
            "GPE  — — —  Midnight Mass\n",
            "GPE  — — —  Christmas\n",
            "PERSON  — — —  Earth\n",
            "PERSON  — — —  Tonight\n",
            "GPE  — — —  Earth\n",
            "GPE  — — —  African\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict_nltk"
      ],
      "metadata": {
        "id": "ifB0nzEYpx1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6aa35b85-66be-4b88-9555-e7f1bc5f7422"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ORGANIZATION': ['LITTLE',\n",
              "  'Sahara',\n",
              "  'International Astronomical Congress',\n",
              "  'Asteroid',\n",
              "  'Come',\n",
              "  'Earth',\n",
              "  'African',\n",
              "  'Good',\n",
              "  'Christmas'],\n",
              " 'PERSON': ['Scared',\n",
              "  'Hence',\n",
              "  'Miles',\n",
              "  'Utterly',\n",
              "  'See',\n",
              "  'Earth',\n",
              "  'Jupiter',\n",
              "  'Mars',\n",
              "  'Venus',\n",
              "  'How',\n",
              "  'Asteroid B-612',\n",
              "  'Children',\n",
              "  'Don',\n",
              "  'Goodbye',\n",
              "  'Him',\n",
              "  'Ashamed',\n",
              "  'Twelve',\n",
              "  'Will',\n",
              "  'Which',\n",
              "  'Siberia',\n",
              "  'Look',\n",
              "  'Hello',\n",
              "  'Correct',\n",
              "  'Wheat',\n",
              "  'Aren',\n",
              "  'Thirst',\n",
              "  'Tonight'],\n",
              " 'GPE': ['China',\n",
              "  'Arizona',\n",
              "  'Turkish',\n",
              "  'European',\n",
              "  'Children',\n",
              "  'America',\n",
              "  'France',\n",
              "  'New Zealand',\n",
              "  'Australia',\n",
              "  'Russia',\n",
              "  'India',\n",
              "  'Africa',\n",
              "  'Europe',\n",
              "  'South America',\n",
              "  'North America',\n",
              "  'Pacific',\n",
              "  'Earth',\n",
              "  'Greetings',\n",
              "  'Sahara',\n",
              "  'Midnight Mass',\n",
              "  'Christmas',\n",
              "  'African'],\n",
              " 'LOCATION': ['Earth', 'North Pole', 'South Pole']}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLTK with StanfordNERTagger"
      ],
      "metadata": {
        "id": "DJjZuFyRiLo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import StanfordNERTagger\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "st = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz',\n",
        "\t\t\t\t\t   'stanford-ner.jar',\n",
        "\t\t\t\t\t   encoding='utf-8')"
      ],
      "metadata": {
        "id": "AXRb6EWJj5oV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Test data'''\n",
        "# text = 'While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.'\n",
        "\n",
        "# tokenized_text = word_tokenize(text)\n",
        "# classified_text = st.tag(tokenized_text)\n",
        "\n",
        "# print(classified_text)"
      ],
      "metadata": {
        "id": "BUowVbSjoEBf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "67f443b8-56fc-4178-b83c-3eab1e8e0231"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Test data'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "entity_3 = []\n",
        "with open(file, \"r\") as file1:\n",
        "  text = file1.read()\n",
        "  # print(len(FileContent)) # 79779\n",
        "\n",
        "  tokenized_text = word_tokenize(text)\n",
        "  classified_text = st.tag(tokenized_text)\n",
        "  entity_3.append(classified_text)\n",
        "  print(classified_text)"
      ],
      "metadata": {
        "id": "EfdpLefUlZ4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_3 = {}\n",
        "for i in range(len(entity_3[0])):\n",
        "  if entity_3[0][i][1] != 'O':\n",
        "    print(entity_3[0][i][0], ' — — — ', entity_3[0][i][1])\n",
        "    if entity_3[0][i][1] not in dict_3.keys():\n",
        "      dict_3[entity_3[0][i][1]] = [entity_3[0][i][0]]\n",
        "    else:\n",
        "      if entity_3[0][i][0] not in dict_3[entity_3[0][i][1]]:\n",
        "        dict_3[entity_3[0][i][1]].append(entity_3[0][i][0])"
      ],
      "metadata": {
        "id": "FiKlnodplixp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c56a91cf-7cca-404d-e228-723ff7ae42db"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China  — — —  LOCATION\n",
            "Arizona  — — —  LOCATION\n",
            "Sahara  — — —  LOCATION\n",
            "Desert  — — —  LOCATION\n",
            "Jupiter  — — —  LOCATION\n",
            "Mars  — — —  LOCATION\n",
            "Venus  — — —  PERSON\n",
            "International  — — —  ORGANIZATION\n",
            "Astronomical  — — —  ORGANIZATION\n",
            "Congress  — — —  ORGANIZATION\n",
            "America  — — —  LOCATION\n",
            "France  — — —  LOCATION\n",
            "France  — — —  LOCATION\n",
            "France  — — —  LOCATION\n",
            "Earth  — — —  LOCATION\n",
            "Earth  — — —  LOCATION\n",
            "Earth  — — —  LOCATION\n",
            "Earth  — — —  LOCATION\n",
            "New  — — —  LOCATION\n",
            "Zealand  — — —  LOCATION\n",
            "Australia  — — —  LOCATION\n",
            "China  — — —  LOCATION\n",
            "Siberia  — — —  LOCATION\n",
            "Russia  — — —  LOCATION\n",
            "India  — — —  LOCATION\n",
            "Africa  — — —  LOCATION\n",
            "Europe  — — —  LOCATION\n",
            "South  — — —  LOCATION\n",
            "America  — — —  LOCATION\n",
            "North  — — —  LOCATION\n",
            "America  — — —  LOCATION\n",
            "Earth  — — —  LOCATION\n",
            "Pacific  — — —  LOCATION\n",
            "Earth  — — —  LOCATION\n",
            "Africa  — — —  LOCATION\n",
            "Earth  — — —  LOCATION\n",
            "Earth  — — —  LOCATION\n",
            "Earth  — — —  LOCATION\n",
            "Earth  — — —  LOCATION\n",
            "Sahara  — — —  LOCATION\n",
            "Earth  — — —  LOCATION\n",
            "Earth  — — —  LOCATION\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spacy"
      ],
      "metadata": {
        "id": "U9EpbZsYoX7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import spacy\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()"
      ],
      "metadata": {
        "id": "1BQZzfitoYrf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tkinter.constants import E\n",
        "dict_spacy = {}\n",
        "with open(file, \"r\") as file1:\n",
        "  text = file1.read()\n",
        "  doc = nlp(text)\n",
        "  for entity in doc.ents:\n",
        "    # print(entity.label_, '— — — ', entity.text)\n",
        "    if entity.label_ not in dict_spacy.keys():\n",
        "      dict_spacy[entity.label_] = [entity.text]\n",
        "    else:\n",
        "      if entity.text not in dict_spacy[entity.label_]:\n",
        "        dict_spacy[entity.label_].append(entity.text)"
      ],
      "metadata": {
        "id": "mVKrZkJJoctb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_spacy['PERSON']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFGKK79IpKUv",
        "outputId": "8b2aaa5f-dc23-467f-f9f4-2c66b236d776"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Miles', 'Asteroid B-612', 'bush', 'Hmm', 'Goodnight']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}